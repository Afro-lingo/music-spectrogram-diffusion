{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxd7xZaAJ38K",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC. All Rights Reserved.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "#@title Setup Environment\n",
        "#@markdown Install *Multi-instrument Music Synthesis with Spectrogram Diffusion* and its dependencies (may take a few minutes).\n",
        "\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 build-essential libasound2-dev libjack-dev\n",
        "\n",
        "!pip install nest-asyncio\n",
        "!pip install pyfluidsynth\n",
        "# pin CLU for python 3.7 compatibility\n",
        "!pip install clu==0.0.7\n",
        "# pin Orbax to use Checkpointer\n",
        "!pip install orbax==0.0.2\n",
        "\n",
        "!pip install note_seq==0.0.3\n",
        "\n",
        "# install t5x\n",
        "!rm -r t5x\n",
        "!git clone --branch=main https://github.com/google-research/t5x\n",
        "# pin T5X for python 3.7 compatibility\n",
        "!cd t5x; git reset --hard 2e05ad41778c25521738418de805757bf2e41e9e; cd ..\n",
        "!mv t5x t5x_tmp; mv t5x_tmp/* .; rm -r t5x_tmp\n",
        "!sed -i 's:jax\\[tpu\\]:jax:' setup.py\n",
        "!python3 -m pip install -e .\n",
        "\n",
        "# install mt3\n",
        "!rm -r mt3\n",
        "!git clone --branch=main https://github.com/magenta/mt3\n",
        "!mv mt3 mt3_tmp; mv mt3_tmp/* .; rm -r mt3_tmp\n",
        "!python3 -m pip install -e .\n",
        "\n",
        "# install music_spectrogram_diffusion\n",
        "!rm -r music_spectrogram_diffusion\n",
        "!git clone --branch=main https://github.com/magenta/music_spectrogram_diffusion\n",
        "!mv music_spectrogram_diffusion music_spectrogram_diffusion_tmp; mv music_spectrogram_diffusion_tmp/* .; rm -r music_spectrogram_diffusion_tmp\n",
        "!python3 -m pip install -e .\n",
        "\n",
        "# copy checkpoints\n",
        "!gsutil -q -m cp gs://music-synthesis-with-spectrogram-diffusion/checkpoints/*.zip .\n",
        "!unzip -o *.zip\n",
        "\n",
        "# copy soundfont (originally from https://sites.google.com/site/soundfonts4u)\n",
        "!gsutil -q -m cp gs://magentadata/soundfonts/SGM-v2.01-Sal-Guit-Bass-V1.3.sf2 ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYMTUO6J3OG8"
      },
      "source": [
        "#Imports and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "PH0kNn_Pdqci"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TSMSWDxxWmTS"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import os\n",
        "\n",
        "import gin\n",
        "import jax\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import tensorflow as tf\n",
        "\n",
        "import librosa\n",
        "import note_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qcMzp5PiPpzU"
      },
      "outputs": [],
      "source": [
        "from music_spectrogram_diffusion.models.diffusion import models\n",
        "from music_spectrogram_diffusion.models.diffusion import network\n",
        "from music_spectrogram_diffusion.models.diffusion import feature_converters\n",
        "from music_spectrogram_diffusion import datasets\n",
        "from music_spectrogram_diffusion import inference\n",
        "from music_spectrogram_diffusion import metrics\n",
        "from music_spectrogram_diffusion import note_sequences\n",
        "from music_spectrogram_diffusion import preprocessors\n",
        "from music_spectrogram_diffusion import run_length_encoding\n",
        "from music_spectrogram_diffusion import audio_codecs\n",
        "from music_spectrogram_diffusion import tasks\n",
        "from music_spectrogram_diffusion import vocabularies\n",
        "\n",
        "import seqio\n",
        "import t5\n",
        "import t5x\n",
        "\n",
        "from t5x import adafactor\n",
        "from t5x import adafactor\n",
        "from t5x import gin_utils\n",
        "from t5x import partitioning\n",
        "from t5x import trainer\n",
        "from t5x import utils\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "SF2_PATH = 'SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_midi():\n",
        "  data = list(files.upload().values())\n",
        "  if len(data) > 1:\n",
        "    print('Multiple files uploaded; using only one.')\n",
        "  return note_seq.midi_to_note_sequence(data[0])"
      ],
      "metadata": {
        "id": "87pZ7Goqb2YX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyTSAD7X4c_O"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y4QXxYlTtWem"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = (\n",
        "  'base_with_context/checkpoint_500000'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ODZcr3Yo6Waj"
      },
      "outputs": [],
      "source": [
        "gin_overrides = [\n",
        "    \"from __gin__ import dynamic_registration\",\n",
        "    \"from music_spectrogram_diffusion.models.diffusion import diffusion_utils\",\n",
        "    \"diffusion_utils.ClassifierFreeGuidanceConfig.eval_condition_weight = 2.0\",\n",
        "    \"diffusion_utils.DiffusionConfig.classifier_free_guidance = @diffusion_utils.ClassifierFreeGuidanceConfig()\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YEgmk_cA5KsP"
      },
      "outputs": [],
      "source": [
        "gin_file = os.path.join(checkpoint_path, '..', 'config.gin')\n",
        "gin_config = inference.parse_training_gin_file(gin_file, gin_overrides)\n",
        "synth_model = inference.InferenceModel(checkpoint_path, gin_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqaQ4qaeLKTt"
      },
      "source": [
        "# Upload MIDI File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgfSyUu9LMyV"
      },
      "outputs": [],
      "source": [
        "ns = upload_midi()\n",
        "\n",
        "note_seq.play_sequence(\n",
        "    ns, synth=note_seq.midi_synth.fluidsynth, sf2_path=SF2_PATH,\n",
        "    sample_rate=SAMPLE_RATE)\n",
        "note_seq.plot_sequence(ns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU1ue6Z7mtsC"
      },
      "source": [
        "# Define Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBIzsGBBmqLz"
      },
      "outputs": [],
      "source": [
        "vocabulary = vocabularies.vocabulary_from_codec(synth_model.codec)\n",
        "note_representation_config = tasks.NoteRepresentationConfig(\n",
        "          onsets_only=False, include_ties=True)\n",
        "\n",
        "# TODO(iansimon): add this to preprocessors.py once it's less hacky\n",
        "def program_to_slakh_program(program):\n",
        "  # this is done very hackily, probably should use a custom mapping\n",
        "  for slakh_program in sorted(preprocessors._SLAKH_CLASS_PROGRAMS.values(),\n",
        "                              reverse=True):\n",
        "    if program >= slakh_program:\n",
        "      return slakh_program\n",
        "\n",
        "def create_dataset(split, shuffle_files):\n",
        "  del split\n",
        "  del shuffle_files\n",
        "  ns_sus = note_seq.apply_sustain_control_changes(ns)\n",
        "  for note in ns_sus.notes:\n",
        "    if not note.is_drum:\n",
        "      note.program = program_to_slakh_program(note.program)\n",
        "  samples = np.zeros(\n",
        "      int(ns_sus.total_time * synth_model.audio_codec.sample_rate))  \n",
        "  frames, frame_times = preprocessors._audio_to_frames(\n",
        "      samples,\n",
        "      synth_model.audio_codec.hop_size,\n",
        "      synth_model.audio_codec.frame_rate)\n",
        "  times, values = (\n",
        "      note_sequences.note_sequence_to_onsets_and_offsets_and_programs(ns_sus))\n",
        "  (events, event_start_indices, event_end_indices,\n",
        "   state_events, state_event_indices) = (\n",
        "       run_length_encoding.encode_and_index_events(\n",
        "           state=note_sequences.NoteEncodingState(),\n",
        "           event_times=times,\n",
        "           event_values=values,\n",
        "           encode_event_fn=note_sequences.note_event_data_to_events,\n",
        "           codec=synth_model.codec,\n",
        "           frame_times=frame_times,\n",
        "           encoding_state_to_events_fn=(\n",
        "               note_sequences.note_encoding_state_to_events)))\n",
        "  return tf.data.Dataset.from_tensors({\n",
        "        'inputs': frames,\n",
        "        'input_times': frame_times.astype(np.float32),\n",
        "        'targets': events.astype(np.int32),\n",
        "        'event_start_indices': event_start_indices.astype(np.int32),\n",
        "        'event_end_indices': event_end_indices.astype(np.int32),\n",
        "        'state_events': state_events.astype(np.int32),\n",
        "        'state_event_indices': state_event_indices.astype(np.int32),\n",
        "        'sequence': ns_sus.SerializeToString()\n",
        "  })\n",
        "\n",
        "infer_task = seqio.Task(\n",
        "    name='infer_full_song',\n",
        "    source=seqio.FunctionDataSource(\n",
        "        create_dataset,\n",
        "        splits=['eval'],\n",
        "        num_input_examples={'eval': 1}),\n",
        "    output_features={\n",
        "        'inputs': seqio.Feature(vocabulary=vocabulary),\n",
        "        'targets': seqio.ContinuousFeature(dtype=tf.float32, rank=2),\n",
        "        'targets_context': seqio.ContinuousFeature(dtype=tf.float32, rank=2),\n",
        "    },\n",
        "    # just take one sequence\n",
        "    preprocessors=tasks.pre_cache_processor_chain(\n",
        "        audio_codec=synth_model.audio_codec,\n",
        "        codec=synth_model.codec,\n",
        "        tokenize_fn=lambda ds, **kwargs: ds,\n",
        "        note_representation_config=note_representation_config,\n",
        "        split_sequences=False\n",
        "    ) + tasks.split_full_song_processor_chain(\n",
        "            audio_codec=synth_model.audio_codec,\n",
        "            feature_context_key='targets_context'\n",
        "    ) + tasks.note_representation_processor_chain(\n",
        "            codec=synth_model.codec,\n",
        "            note_representation_config=note_representation_config\n",
        "    ) + [\n",
        "        functools.partial(\n",
        "            preprocessors.encode_audio,\n",
        "            targets_keys=['targets'],\n",
        "            context_keys=['targets_context'],\n",
        "            keys_to_pad=None,\n",
        "            audio_codec=synth_model.audio_codec),\n",
        "        functools.partial(preprocessors.handle_too_long, skip=False),\n",
        "        functools.partial(\n",
        "            seqio.preprocessors.tokenize_and_append_eos,\n",
        "            copy_pretokenized=True)\n",
        "    ])\n",
        "\n",
        "infer_ds = infer_task.get_dataset(\n",
        "    sequence_length=synth_model.sequence_length,\n",
        "    split='eval',\n",
        "    use_cached=False, shuffle=False)\n",
        "\n",
        "infer_ds_raw_batches = infer_ds.batch(1)\n",
        "infer_ds_feats = synth_model.model.FEATURE_CONVERTER_CLS(pack=False)(\n",
        "    infer_ds, synth_model.sequence_length)\n",
        "infer_ds_batches = infer_ds_feats.batch(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nnMCFazdehA"
      },
      "source": [
        "# Synthesize Entire Song"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0izOmp4feP7i"
      },
      "outputs": [],
      "source": [
        "# Initialize previous prediction to zeros.\n",
        "# We'll zero out the mask for the first prediction, so the value here doesn't\n",
        "# matter.\n",
        "pred_mel = np.zeros([1, synth_model.sequence_length['targets_context'],\n",
        "                     synth_model.audio_codec.n_dims])\n",
        "\n",
        "# Variables for accumulating the full song prediction.\n",
        "full_pred_mel = np.zeros([1, 0, synth_model.audio_codec.n_dims], np.float32)\n",
        "\n",
        "# Iterators over the dataset.\n",
        "infer_ds_batches_iter = infer_ds_batches.as_numpy_iterator()\n",
        "\n",
        "i = 0\n",
        "while True:\n",
        "  try:\n",
        "    batch = next(infer_ds_batches_iter)\n",
        "    # TODO(fjord): put the first-chunk logic into a library function\n",
        "    batch['encoder_continuous_inputs'] = pred_mel[:1]\n",
        "    if i == 0:\n",
        "      # The first chunk has no previous context.\n",
        "      batch['encoder_continuous_mask'] = np.zeros_like(\n",
        "          batch['encoder_continuous_mask'])\n",
        "    else:\n",
        "      # The full song pipeline does not feed in a context feature, so the mask\n",
        "      # will be all 0s after the feature converter. Because we know we're\n",
        "      # feeding in a full context chunk from the previous prediction, set it\n",
        "      # to all 1s.\n",
        "      batch['encoder_continuous_mask'] = np.ones_like(\n",
        "          batch['encoder_continuous_mask'])\n",
        "    # it wants batch size divisible by 8 on TPU...\n",
        "    for key in batch:\n",
        "      batch[key] = np.tile(batch[key], [8] + [1] * (batch[key].ndim - 1))\n",
        "    pred_mel, scores = synth_model.predict(batch)\n",
        "    full_pred_mel = np.concatenate([full_pred_mel, pred_mel[:1]], axis=1)\n",
        "    i += 1\n",
        "    print('generated %d segments' % i)\n",
        "  except StopIteration:\n",
        "    break\n",
        "\n",
        "full_pred_audio = synth_model.audio_codec.decode(full_pred_mel)\n",
        "\n",
        "plt.matshow(np.rot90(full_pred_mel[0]))\n",
        "play(full_pred_audio)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Synthesis from MIDI",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}